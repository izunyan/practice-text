---
title: "practice_text"
author: ""
date: "`r format(Sys.time(), '%Y/%m/%d')`"
output:
  html_document: 
    toc: TRUE
    toc_float: true
    toc_depth: 4
    number_sections: true
    theme: readable
    highlight: pygments
editor_options: 
  chunk_output_type: console
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

# パッケージ読み込み
```{r}
library(tidyverse)
library(quanteda)
```

# 参考
* [クイック・スタートガイド](https://quanteda.io/articles/pkgdown/quickstart_ja.html)
* [quanteda tutorials](https://tutorials.quanteda.io/multilingual/japanese/)
* [経済学における量的テキスト分析入門](https://github.com/koheiw/workshop-JEAT)
* [Rによる日本語のテキスト分析入門](https://github.com/koheiw/workshop-IJTA)


# 大きな文章を読み込んで分割
## 読み込み

* 太宰治[『女生徒』]　青空文庫 (https://www.aozora.gr.jp/cards/000035/card275.html)

```{r}
# 1行ごと読み込む関数だが，青空文庫の仕様上，段落（１行？）ごとに読み込み？
dat_txt <- 
readLines("data/275_ruby_1532/joseito.txt")

dat_txt %>%
  head(20)

# 文章開始までの部分を削除
dat_txt <- 
  dat_txt[-(1:17)]

dat_txt %>%
  tail(20)

# 文章最終文の後の部分を削除
dat_txt <- 
  dat_txt[-(96:107)]

dat_txt %>%
  tail()  

 # dat_txt <- readtext::readtext("data/275_ruby_1532/joseito.txt", cache = FALSE)

```

### ルビを取り除く

* https://mjin.doshisha.ac.jp/R/57/57.html
  + "[^》]+"は、記号"》"以外のすべての文字列

```{r}
# 除去前
dat_txt[1]

dat_txt <- 
  dat_txt %>% 
  str_remove_all("《[^》]+》")

# 除去後
dat_txt[1]
```


## コーパス化
```{r}
corp_dat <- 
  corpus(dat_txt)

corp_dat %>% head()

# 5つ目までのsummary
summary(corp_dat, 5)
```

### 文ごとに変換
```{r}
corpus_reshape(corp_dat, to = "sentences") %>% 
summary(5)
```

### タグ付けでデータフレームに抽出
```{r}
corp_tagged <- corpus(c(
  # text1
  "##最初の文 あさ、眼をさますときの気持は、面白い。
   ##セリフ 「見つけた！」",
  # text2
  "",
  # text3
  "##感情 へんに気恥ずかしく、うれしく、 ##セリフ  よいしょ、と掛声して、"))
corp_sect <- corpus_segment(corp_tagged, pattern = "##*")


cbind(docvars(corp_sect), text = as.character(corp_sect))
```






## トークン化
### 基本の例
```{r}
txt <- c(text1 = "あさ、眼をさますときの気持は、面白い。",
         text2 = "朝は灰色。いつもいつも同じ。",
         text3 = "(1)これはテストの文章")

tokens(txt)
```

#### 句読点などを削除
```{r}
tokens(txt, remove_punct = TRUE, remove_numbers = TRUE)
```

## 文書行列（dfm）


### （不使用）文単位に分割
```{r eval=FALSE, include=FALSE}
# 文単位に分割
dat_txt_sent <- 
  dat_txt %>% 
  str_split(pattern = "(?<=。)") %>% 
  unlist()

dat_txt_sent %>% head()

```



# 参考（その他）

* [青空文庫からファイルを（半）自動ダウンロードでテキストマイニング（したい？）](http://rstudio-pubs-static.s3.amazonaws.com/3345_a88bc1244a08425d95772d0418f71048.html)


