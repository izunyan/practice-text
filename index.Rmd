---
title: "practice_text"
author: "やわらかクジラ"
date: "`r format(Sys.time(), '%Y/%m/%d')`"
output:
  html_document: 
    toc: TRUE
    toc_float: true
    toc_depth: 4
    number_sections: true
    theme: readable
    highlight: pygments
editor_options: 
  chunk_output_type: console
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

# 参考
## 全般
* [Text Mining with R: A Tidy Approach (Julia Silge and David Robinson)](https://www.tidytextmining.com/index.html)

## quanteda
* [クイック・スタートガイド](https://quanteda.io/articles/pkgdown/quickstart_ja.html)
* [quanteda tutorials](https://tutorials.quanteda.io/multilingual/japanese/)
* [経済学における量的テキスト分析入門](https://github.com/koheiw/workshop-JEAT)
* [Rによる日本語のテキスト分析入門](https://github.com/koheiw/workshop-IJTA)
* [フリーソフトによるデータ解析・マイニング](https://www.cis.doshisha.ac.jp/mjin/R/index.html)

## 用語

* 形態素 morpheme
* 形態素解析 morphological analysis
* 分かち書き word segmentation

# 準備
## パッケージ読み込み
```{r}
library(tidyverse)
library(quanteda)
```


<!-- # テキストデータ基本 -->
<!-- ##  -->

# 大きな文章を読み込んで分割
## 読み込み

* 太宰治[『女生徒』](https://www.aozora.gr.jp/cards/000035/card275.html)青空文庫
* ここではダウンロードして回答したフォルダ`275_ruby_1532`をdataフォルダの中に置く

```{r}
# 1行ごと読み込む関数だが，青空文庫の仕様上，段落（１行？）ごとに読み込み？
dat_txt <-
  readr::read_lines("data/275_ruby_1532/joseito.txt", 
                    locale = locale(encoding = "cp932"))

dat_txt |>
  head(20)

# 文章開始までの部分を削除
dat_txt <- 
  dat_txt[-(1:17)]

dat_txt |>
  tail(20)

# 文章最終文の後の部分を削除
dat_txt <- 
  dat_txt[-(96:107)]

dat_txt |>
  tail()  

 # dat_txt <- readtext::readtext("data/275_ruby_1532/joseito.txt", cache = FALSE)

```

### ルビを取り除く

* https://mjin.doshisha.ac.jp/R/57/57.html
  + "[^》]+"は、記号"》"以外のすべての文字列

```{r}
# 除去前
dat_txt[1]

dat_txt <- 
  dat_txt |> 
  str_remove_all("《[^》]+》")

# 除去後
dat_txt[1]
```


# コーパス化
```{r}
corp_dat <- 
  corpus(dat_txt)

corp_dat |> head()

# 5つ目までのsummary
summary(corp_dat, 5)
```

## 文ごとに変換
```{r}
corpus_reshape(corp_dat, to = "sentences") |> 
summary(5)
```

## タグ付けでデータフレームに抽出
```{r}
corp_tagged <- corpus(c(
  # text1
  "##最初の文 あさ、眼をさますときの気持は、面白い。
   ##セリフ 「見つけた！」",
  # text2
  "",
  # text3
  "##感情 へんに気恥ずかしく、うれしく、 ##セリフ  よいしょ、と掛声して、"))
corp_sect <- corpus_segment(corp_tagged, pattern = "##*")


cbind(docvars(corp_sect), text = as.character(corp_sect))
```



# トークン化
* すでに各行に文章が入っているデータフレームはここから
* version 3以降は，コーパス内検索するのもdfm化するのもまずトークン化が必須に

## 基本の例
```{r}
txt <- c(text1 = "あさ、眼をさますときの気持は、面白い。",
         text2 = "朝は灰色。いつもいつも同じ。",
         text3 = "(1)これはテストの文章")

tokens(txt)
```

### 句読点などを削除
```{r}
tokens(txt, remove_punct = TRUE, remove_numbers = TRUE)
```

## コーパスのトークン化
```{r}
tok_dat <- 
tokens(corp_dat, remove_punct = TRUE)

tok_dat |> head()
```

## 精緻なトークン化
### 共起関係把握
```{r}
library(quanteda.textstats)

# 全体
textstat_collocations(corp_dat) |> head(10)




min_count = 2 # 2回以上登場

# 漢字のみ
kanji_col <- tokens_select(tok_dat, "^[一-龠]+$", valuetype = "regex", padding = TRUE) |> 
             textstat_collocations() # min_count = min_count
kanji_col

tok_dat <- tokens_compound(tok_dat, kanji_col[kanji_col$z > 3,], concatenator = "")


# カタカナのみ
kana_col <- tokens_select(tok_dat, "^[ァ-ヶー]+$", valuetype = "regex", padding = TRUE) |> 
            textstat_collocations()
kana_col

tok_dat <- 
tokens_compound(tok_dat, kana_col[kana_col$z > 3,], concatenator = "")


# 漢字とカタカナと数字
any_col <- 
tokens_select(tok_dat, "^[０-９ァ-ヶー一-龠]+$", valuetype = "regex", padding = TRUE) |> 
           textstat_collocations()

any_col

tok_dat <- 
tokens_compound(tok_dat, any_col[any_col$z > 3,], concatenator = "")

# 漢字とひらがな
# kanhira_col <- 
#   tokens_select(tok_dat, "^[一-龠ぁ-ん]+$", valuetype = "regex", padding = TRUE) |> 
#            textstat_collocations()
# 
# kanhira_col

# ひらがなのみ（反映せず）
# hira_col <- tokens_select(tok_dat, "^[ぁ-ん]+$", valuetype = "regex", padding = TRUE) |> 
#             textstat_collocations(min_count = 10)
# hira_col |> head()

# 多い順
# kanhira_col |> arrange(desc(count))

# corp_dat <- 
# tokens_compound(corp_dat, kana_col[kana_col$z > 3,], concatenator = "")

```

### 指定した語をくっつける

* [ドキュメント](https://quanteda.io/reference/tokens_compound.html)やヘルプのexample参照

```{tokens_compound r}

jisho <- 
  list(c("あ", "さ"),
       c("さ", "ます"))

tokens_compound(tok_dat, jisho,  concatenator = "") |>
  head(1)

# 他の候補
       # c("私", "たち"),
       # c("笑", "って"),
       # c("読", "んで"),
       # c("黙", "って"),
       # |> c("新", "ちゃん")

```


## 検索
```{r}
kwic(tok_dat, pattern = "眼鏡")
kwic(tok_dat, pattern = "小さい")
kwic(tok_dat, pattern = "植木屋")

```

## stopwords

https://tutorials.quanteda.io/multilingual/japanese/

```{r}
tok_dat <- 
  tok_dat |> 
  tokens_remove(pattern = stopwords("ja", source = "marimo"), verbose = TRUE)
```



# 文書行列（dfm）

* dfm:document-feature matrix
* 行が文書（document），列が特長（feature）となる行列

```{r}
dfm_dat <- 
dfm(tok_dat)

dfm_dat
```

## 頻度の確認
```{r}
# top20
topfeatures(dfm_dat, 10)

# 下位20
topfeatures(dfm_dat, decreasing = FALSE, 10)

```


### 頻度が低い語と高い語を削除

* ひらがなのみは`"^[ぁ-ん]+$"`

```{r}
dfm_dat_s <- 
  dfm_dat |> 
  dfm_remove("^[ぁ-ん]{1,4}$", valuetype = "regex", # ひらがな1～4字は削除
              min_nchar = 2,      # 2字未満は削除
              verbose = TRUE) # |> # 除去したfeature数を表示
  # dfm_trim(min_termfreq = 0.50,  
  #          termfreq_type = "quantile", 
  #          max_termfreq = 0.99)

# 上位20
topfeatures(dfm_dat_s, 10)

# 下位20
topfeatures(dfm_dat_s, decreasing = FALSE, 10)

# 頻度が多い単語も除去
dfm_dat_s2 <- 
  dfm_dat_s |> 
  dfm_trim(min_termfreq = 0.50,
           termfreq_type = "quantile",
           max_termfreq = 0.995)

```


```{r}
# 10回以上, 2以上のdocumentsで登場
dfm_dat_s |> 
  dfm_trim(min_termfreq = 10, 
           min_docfreq = 2, 
           verbose = TRUE) |> 
  topfeatures(30)

# 10回以下，2以下のdocumantsで登場（局所的）
dfm_dat_s |> 
  dfm_trim(max_termfreq = 10, 
           max_docfreq = 2, 
           verbose = TRUE) |> 
  topfeatures(30)


# 5回以上, 40%以上のdocumentsで登場
# dfm_dat_s |> 
#   dfm_trim(min_termfreq = 5, 
#            min_docfreq = 0.4,
#            verbose = TRUE) |> 
#   topfeatures(30)






dfm_dat_s |> 
   dfm_trim(min_termfreq = 0.50,        # top 50%?
            termfreq_type = "quantile", 
            max_termfreq = 0.99, # 下位99%　→　top1%は除く
            verbose = TRUE) |>    
  topfeatures(10)


# 除去される語はなし？
dfm_dat_s |> 
   dfm_trim(min_termfreq = 0.50,  
            termfreq_type = "quantile",
            verbose = TRUE) |> 
  topfeatures(10)

dfm_dat_s |> 
   dfm_trim(max_termfreq = 0.99,  
            termfreq_type = "quantile",
            verbose = TRUE) |> 
  topfeatures(10)
 

```



# 分析
## 相対頻度

* デフォルトでは'target = 1L'となっているので，text1を分析するようになっている

```{r}
textstat_keyness(dfm_dat_s) |> 
head(20)


textstat_keyness(dfm_dat_s, target = 14L) |> 
head(20)

```

## ワードクラウド
### 頻度の制限なし
```{r}
library(quanteda.textplots)

set.seed(123)
textplot_wordcloud(dfm_dat_s,
                   random_order = FALSE,
                   rotation = .25,
                   min_count = 5,
                   min_size = 1,
                   max_size = 5,
                   color = RColorBrewer::brewer.pal(8, "Dark2"))
```

### 高頻度語を除去
```{r}

set.seed(123)
textplot_wordcloud(dfm_dat_s2,
                   random_order = FALSE,
                   rotation = .25,
                   min_count = 5,
                   min_size = 1,
                   max_size = 5,
                   color = RColorBrewer::brewer.pal(8, "Dark2"))
```


## 共起ネットワーク
### 頻度の制限なし
```{r}



dfm_fcm <- fcm(dfm_dat_s)
feat <- names(topfeatures(dfm_fcm, 100))
dfm_dat_col <- fcm_select(dfm_fcm, feat)
textplot_network(dfm_dat_col, min_freq = 0.95, edge_size = 5)


# dfm_fcm <- 
#   dfm_select(dfm_dat, feat) |> 
#   fcm()
# 
# size <- 
#   sqrt(rowSums(dfm_fcm))
# 
# textplot_network(dfm_fcm, min_freq = 0.85, edge_alpha = 0.9, 
#                  vertex_size = size / max(size) * 3,
#                  vertex_labelfont = if (Sys.info()["sysname"] == "Darwin") "SimHei" else NULL)
```

### 高頻度語を除去
```{r}
dfm_fcm2 <- fcm(dfm_dat_s2)
feat2 <- names(topfeatures(dfm_fcm2, 100))
dfm_dat_col2 <- fcm_select(dfm_fcm2, feat2)
textplot_network(dfm_dat_col2, min_freq = 0.95, edge_size = 5)
```


## トピックモデル
```{r eval=FALSE, include=FALSE}
library(topicmodels)

set.seed(100)
lda <- LDA(convert(dfm_fcm, to = "topicmodels"), k = 10)

# 表示
get_terms(lda, 10) |> knitr::kable()

# LDA_fit_10 <- convert(dfm_fcm, to = "topicmodels") |> 
#     LDA(k = 10)
```












## （不使用）文単位に分割
```{r eval=FALSE, include=FALSE}
# 文単位に分割
dat_txt_sent <- 
  dat_txt |> 
  str_split(pattern = "(?<=。)") |> 
  unlist()

dat_txt_sent |> head()

```



# 参考（その他）

* [青空文庫からファイルを（半）自動ダウンロードでテキストマイニング（したい？）](http://rstudio-pubs-static.s3.amazonaws.com/3345_a88bc1244a08425d95772d0418f71048.html)

# 他のテキスト取得方法
## web上から

* 太宰治[『女生徒』](https://www.aozora.gr.jp/cards/000035/files/275_13903.html)XHTML版　青空文庫

### webページをRに読み込み

```{r}
library(rvest)

txt <- read_html("https://www.aozora.gr.jp/cards/000035/files/275_13903.html")
```

### 文字情報を取得

* `read_html()`で読み込んだオブジェクトから文字情報を取得
* テキストデータがひとかたまりで取得され，そのまま表示させると膨大な出力になるので，`str_sub()`で一部を確認

```{r}
txt_html <- 
  txt |> 
  html_text()

txt_html|> 
  str_sub(1,500) # 1文字目から500文字目まで表示
```


### 加工
#### 改行単位で区切る

* `str_split()`を使って，指定した区切り（ここでは改行）ごとに分割する。改行部分は`\r\n`
* 分割した結果はリストになっているので，リストの１要素目（"[[1]]"）を取り出す

```{r}
txt_split <- 
  txt_html |> 
  str_split("\r\n")

# 分割結果がリストになっているので抽出
txt_split <- 
  txt_split[[1]]

# 最初の8個の分割確認
txt_split[1:8]

# 元データ削除
rm(txt)
```





#### 本文以外の情報を削除

* 本文以外の情報が分割結果のどこにあるか手動で確認

```{r}
txt_split |>
  as_tibble() |> 
  slice_head(n = 10)


txt_split |>
  as_tibble() |> 
  mutate(id = row_number()) |> # 行番号を示すためにid作成
  slice_tail(n = 20)

```

* 本文部分だけに限定

```{r}
txt_split <- 
  txt_split[7:101]
```

* 確認

```{r}
txt_split |>
  as_tibble() |> 
  slice_head(n = 3)


txt_split |>
  as_tibble() |> 
  slice_tail(n = 3)

```



#### ルビを取り除く

* ただし，（　）の部分が全て削除になるので，読みがな以外の（　）を残す工夫が必要

#### 確認
```{r}
# 除去前
txt_split[1]


```

#### すべての（）を確認 
```{r}
yomigana <- 
  txt_split |> 
  str_extract_all("（[^）]+）") |> 
  flatten_chr()

yomigana |> 
  head()

# df_yomigana <- 
#   yomigana |> 
#   as_tibble()
# 
# writexl::write_xlsx(df_yomigana, "out/df_yomigana.xlsx")
```

#### 漢字があるかどうか確認
```{r}
yomigana |> 
  str_subset("[一-龠]")
```

##### 個別に置換
```{r}
txt_split <- 
  txt_split |> 
  str_replace_all(c("（可哀想（かわいそう）" = "（可哀想",
                    "（口だけでは、やれ古いのなんのって言うけれども、決して人生の先輩、老人、既婚の人たちを軽蔑なんかしていない。それどころか、いつでも二目（にもく）" =
                      "（口だけでは、やれ古いのなんのって言うけれども、決して人生の先輩、老人、既婚の人たちを軽蔑なんかしていない。それどころか、いつでも二目"))
  
```

* 確認

```{r}
yomigana <- 
  txt_split |> 
  str_extract_all("（[^）]+）") |> 
  flatten_chr()

yomigana |> 
  str_subset("[一-龠]")
```

* 再度置換

```{r}

txt_split <- 
  txt_split |> 
  str_replace_all(c(
"（口だけでは、やれ古いのなんのって言うけれども、決して人生の先輩、老人、既婚の人たちを軽蔑なんかしていない。それどころか、いつでも二目も三目（さんもく）" =
"（口だけでは、やれ古いのなんのって言うけれども、決して人生の先輩、老人、既婚の人たちを軽蔑なんかしていない。それどころか、いつでも二目も三目")) 

yomigana <- 
  txt_split |> 
  str_extract_all("（[^）]+）") |> 
  flatten_chr()

yomigana |> 
  str_subset("[一-龠]")
```

#### 削除するよみがなの一覧を作成

* dplyr::setdiffで削除する要素を指定

```{r}

kanji <-
  yomigana |>
  str_subset("[一-龠]")

yomigana <- 
  yomigana |> 
  setdiff(kanji)
  



# 動かない
# kanji <- 
#   yomigana |> 
#   str_subset("[一-龠]")
# 
# yomigana <- 
#   yomigana |> 
#   str_remove_all(all_of(kanji), "")
```


#### (未完成)漢字を含むもの以外を確認
```{r}
yomigana |> 
  str_subset("[^一-龠]")

```


#### 句読点があるかどうか確認
```{r}
yomigana |> 
  str_subset("[、。]")

```

#### (未完成)ひらがなのみを確認
```{r}
yomigana |> 
  str_subset("（[ぁ-ん]+）")


```


#### 指定したよみがなを削除

* 

```{r}
txt_split <- 
  txt_split |> 
  str_remove_all(str_c(yomigana, collapse = "|"))



# 除去後
txt_split[1]
```