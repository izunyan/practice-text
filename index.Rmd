---
title: "practice_text"
author: ""
date: "`r format(Sys.time(), '%Y/%m/%d')`"
output:
  html_document: 
    toc: TRUE
    toc_float: true
    toc_depth: 4
    number_sections: true
    theme: readable
    highlight: pygments
editor_options: 
  chunk_output_type: console
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

# 準備
## パッケージ読み込み
```{r}
library(tidyverse)
library(quanteda)
```

## 参考
* [クイック・スタートガイド](https://quanteda.io/articles/pkgdown/quickstart_ja.html)
* [quanteda tutorials](https://tutorials.quanteda.io/multilingual/japanese/)
* [経済学における量的テキスト分析入門](https://github.com/koheiw/workshop-JEAT)
* [Rによる日本語のテキスト分析入門](https://github.com/koheiw/workshop-IJTA)


# 大きな文章を読み込んで分割
## 読み込み

* 太宰治[『女生徒』](https://www.aozora.gr.jp/cards/000035/card275.html)青空文庫

```{r}
# 1行ごと読み込む関数だが，青空文庫の仕様上，段落（１行？）ごとに読み込み？
dat_txt <- 
readLines("data/275_ruby_1532/joseito.txt")

dat_txt %>%
  head(20)

# 文章開始までの部分を削除
dat_txt <- 
  dat_txt[-(1:17)]

dat_txt %>%
  tail(20)

# 文章最終文の後の部分を削除
dat_txt <- 
  dat_txt[-(96:107)]

dat_txt %>%
  tail()  

 # dat_txt <- readtext::readtext("data/275_ruby_1532/joseito.txt", cache = FALSE)

```

### ルビを取り除く

* https://mjin.doshisha.ac.jp/R/57/57.html
  + "[^》]+"は、記号"》"以外のすべての文字列

```{r}
# 除去前
dat_txt[1]

dat_txt <- 
  dat_txt %>% 
  str_remove_all("《[^》]+》")

# 除去後
dat_txt[1]
```


# コーパス化
```{r}
corp_dat <- 
  corpus(dat_txt)

corp_dat %>% head()

# 5つ目までのsummary
summary(corp_dat, 5)
```

## 文ごとに変換
```{r}
corpus_reshape(corp_dat, to = "sentences") %>% 
summary(5)
```

## タグ付けでデータフレームに抽出
```{r}
corp_tagged <- corpus(c(
  # text1
  "##最初の文 あさ、眼をさますときの気持は、面白い。
   ##セリフ 「見つけた！」",
  # text2
  "",
  # text3
  "##感情 へんに気恥ずかしく、うれしく、 ##セリフ  よいしょ、と掛声して、"))
corp_sect <- corpus_segment(corp_tagged, pattern = "##*")


cbind(docvars(corp_sect), text = as.character(corp_sect))
```



# トークン化

* version 3以降は，コーパス内検索するのもdfm化するのもまずトークン化が必須に

## 基本の例
```{r}
txt <- c(text1 = "あさ、眼をさますときの気持は、面白い。",
         text2 = "朝は灰色。いつもいつも同じ。",
         text3 = "(1)これはテストの文章")

tokens(txt)
```

### 句読点などを削除
```{r}
tokens(txt, remove_punct = TRUE, remove_numbers = TRUE)
```

## コーパスのトークン化
```{r}
tok_dat <- 
tokens(corp_dat, remove_punct = TRUE)

tok_dat %>% head()
```

## 精緻なトークン化
### 共起関係把握
```{r}
library(quanteda.textstats)

# 全体
textstat_collocations(corp_dat) %>% head(10)




min_count = 2 # 2回以上登場

# 漢字のみ
kanji_col <- tokens_select(tok_dat, "^[一-龠]+$", valuetype = "regex", padding = TRUE) %>% 
             textstat_collocations() # min_count = min_count
kanji_col

tok_dat <- tokens_compound(tok_dat, kanji_col[kanji_col$z > 3,], concatenator = "")


# カタカナのみ
kana_col <- tokens_select(tok_dat, "^[ァ-ヶー]+$", valuetype = "regex", padding = TRUE) %>% 
            textstat_collocations()
kana_col

tok_dat <- 
tokens_compound(tok_dat, kana_col[kana_col$z > 3,], concatenator = "")


# 漢字とカタカナと数字
any_col <- 
tokens_select(tok_dat, "^[０-９ァ-ヶー一-龠]+$", valuetype = "regex", padding = TRUE) %>% 
           textstat_collocations()

tok_dat <- 
tokens_compound(tok_dat, any_col[any_col$z > 3,], concatenator = "")


# ひらがなのみ（反映せず）
hira_col <- tokens_select(tok_dat, "^[ぁ-ん]+$", valuetype = "regex", padding = TRUE) %>% 
            textstat_collocations(min_count = 10)
hira_col %>% head()

# corp_dat <- 
# tokens_compound(corp_dat, kana_col[kana_col$z > 3,], concatenator = "")

```

### 指定した語をくっつける
```{r}

jisho <- 
  list(keys1 = c("あ", "さ"),
       keys2 = c("さ", "ます"))

tokens_compound(tok_dat, jisho,  concatenator = "") %>%
  head(1)

```


## 検索
```{r}
kwic(tok_dat, pattern = "眼鏡")
kwic(tok_dat, pattern = "小さい")
kwic(tok_dat, pattern = "植木屋")

```



# 文書行列（dfm）

* dfm:document-feature matrix
* 行が文書（document），列が特長（feature）となる行列

```{r}
dfm_dat <- 
dfm(tok_dat)

dfm_dat
```

## 頻度の確認
```{r}
# top20
topfeatures(dfm_dat, 10)

# 下位20
topfeatures(dfm_dat, decreasing = FALSE, 10)

```


### 頻度が低い語と高い語を削除

* ひらがなのみは`"^[ぁ-ん]+$"`

```{r}
dfm_dat_s <- 
  dfm_dat %>% 
  dfm_remove("^[ぁ-ん]{1,4}$", valuetype = "regex", # ひらがな1～4字は削除
              min_nchar = 2,      # 2字未満は削除
             verbose = TRUE) %>% # 除去したfeature数を表示
  dfm_trim(min_termfreq = 0.50,  
           termfreq_type = "quantile", 
           max_termfreq = 0.99)

# 上位20
topfeatures(dfm_dat_s, 10)

# 下位20
topfeatures(dfm_dat_s, decreasing = FALSE, 10)
```


# 分析
## 相対頻度

* デフォルトでは'target = 1L'となっているので，text1を分析するようになっている

```{r}
textstat_keyness(dfm_dat_s) %>% 
head(20)


textstat_keyness(dfm_dat_s, target = 14L) %>% 
head(20)

```

## ワードクラウド
```{r}
library(quanteda.textplots)

set.seed(123)
textplot_wordcloud(dfm_dat_s,
                   random_order = FALSE,
                   rotation = .25,
                   color = RColorBrewer::brewer.pal(8, "Dark2"))
```


## 共起ネットワーク
```{r}



dfm_fcm <- fcm(dfm_dat_s)
feat <- names(topfeatures(dfm_fcm, 100))
dfm_dat_col <- fcm_select(dfm_fcm, feat)
textplot_network(dfm_dat_col, min_freq = 0.95, edge_size = 5)


# dfm_fcm <- 
#   dfm_select(dfm_dat, feat) %>% 
#   fcm()
# 
# size <- 
#   sqrt(rowSums(dfm_fcm))
# 
# textplot_network(dfm_fcm, min_freq = 0.85, edge_alpha = 0.9, 
#                  vertex_size = size / max(size) * 3,
#                  vertex_labelfont = if (Sys.info()["sysname"] == "Darwin") "SimHei" else NULL)
```

## トピックモデル
```{r eval=FALSE, include=FALSE}
library(topicmodels)

set.seed(100)
lda <- LDA(convert(dfm_fcm, to = "topicmodels"), k = 10)

# 表示
get_terms(lda, 10) %>% knitr::kable()

# LDA_fit_10 <- convert(dfm_fcm, to = "topicmodels") %>% 
#     LDA(k = 10)
```












## （不使用）文単位に分割
```{r eval=FALSE, include=FALSE}
# 文単位に分割
dat_txt_sent <- 
  dat_txt %>% 
  str_split(pattern = "(?<=。)") %>% 
  unlist()

dat_txt_sent %>% head()

```



# 参考（その他）

* [青空文庫からファイルを（半）自動ダウンロードでテキストマイニング（したい？）](http://rstudio-pubs-static.s3.amazonaws.com/3345_a88bc1244a08425d95772d0418f71048.html)


